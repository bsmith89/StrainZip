{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "_os.chdir(_os.environ['PROJECT_ROOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import strainzip as sz\n",
    "import graph_tool as gt\n",
    "import graph_tool.draw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "import xarray as xr\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from strainzip.pandas_util import idxwhere\n",
    "from graph_tool.util import find_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs = True\n",
    "\n",
    "length_bins = np.logspace(0, 6.5, num=51)\n",
    "depth_bins = np.logspace(-1, 4, num=51)\n",
    "\n",
    "k = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth data\n",
    "depth_table = xr.load_dataarray(f'examples/xjin_test4/r.proc.kmtricks-k{k}-m3-r2.ggcat.unitig_depth.nc')\n",
    "depth_table.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'examples/xjin_test4/r.proc.kmtricks-k{k}-m3-r2.ggcat.fn') as f:\n",
    "    _, seqs = sz.io.load_graph_and_sequences_from_linked_fasta(f, k=k, header_tokenizer=sz.io.ggcat_header_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph\n",
    "graph = sz.io.load_graph(f'examples/xjin_test4/r.proc.kmtricks-k{k}-m3-r2.ggcat.gt')\n",
    "# FIXME: These annotations should go into the loading app:\n",
    "graph.gp['num_samples'] = graph.new_graph_property('int', val=depth_table.sizes['sample'])\n",
    "graph.gp['kmer_length'] = graph.new_graph_property('int', val=k)\n",
    "\n",
    "# Set depth on graph\n",
    "vertex_unitig_order = [int(s[:-1]) for s in graph.vp['sequence']]\n",
    "graph.vp['depth'] = graph.new_vertex_property('vector<float>')\n",
    "graph.vp['depth'].set_2d_array(depth_table.sel(unitig=vertex_unitig_order).T.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select components in a deterministic way (from largest to smallest).\n",
    "\n",
    "component_graphs = []\n",
    "\n",
    "graph_remaining = graph.new_vertex_property('bool', val=True)\n",
    "\n",
    "last_graph_size = 1_000_000\n",
    "while last_graph_size > 1000:\n",
    "    this_component = gt.topology.label_largest_component(gt.GraphView(graph, vfilt=graph_remaining), directed=False)\n",
    "    component_graphs.append(gt.GraphView(graph, vfilt=this_component))\n",
    "    graph_remaining = graph.new_vertex_property('bool', vals=graph_remaining.a - this_component.a)\n",
    "    last_graph_size = this_component.a.sum()\n",
    "\n",
    "len(component_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The largest components has a huge fraction of the unitigs\n",
    "component_graphs[0], component_graphs[1], component_graphs[2], component_graphs[3], component_graphs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "# component = c\n",
    "component = 12  # Only the label for plotting\n",
    "\n",
    "graph2 = gt.Graph(component_graphs[c], prune=True)\n",
    "# graph2.ep['filter'] = graph2.new_edge_property('bool',   # TODO: Think about filtering edges instead of removing them entirely.\n",
    "graph2.set_vertex_filter(graph2.vp['filter'])\n",
    "\n",
    "np.random.seed(1)\n",
    "gt.seed_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Start tracking twin-unitigs\n",
    "\n",
    "# def find_twins(graph, segment_to_sequence):\n",
    "#     # Find all twins on the initial graph\n",
    "#     d0 = sz.results.extract_vertex_data(graph, segment_to_sequence=segment_to_sequence)\n",
    "#     twins = []\n",
    "#     for vertex, d1 in d0.iterrows():\n",
    "#         reversed_twin_segments = []\n",
    "#         for segment in d1.segments:\n",
    "#             unitig, strand = segment[:-1], segment[-1]\n",
    "#             opposite_strand = {'+': '-', '-': '+'}[strand]\n",
    "#             reversed_twin_segments.append(f'{unitig}{opposite_strand}')\n",
    "#         twin_segments = tuple(reversed(reversed_twin_segments))\n",
    "#         twin_vertices = idxwhere(d0.segments == twin_segments)\n",
    "#         assert len(twin_vertices) == 1\n",
    "#         twins.append((vertex, twin_vertices[0]))\n",
    "\n",
    "#     return twins\n",
    "\n",
    "# def check_twins(twins, graph, segment_to_sequence):\n",
    "#     d0 = sz.results.extract_vertex_data(graph, segment_to_sequence)\n",
    "#     # Check that all twins have the same info:\n",
    "#     for i, j in twins:\n",
    "#         a = d0.loc[i]\n",
    "#         b = d0.loc[j]\n",
    "#         assert (len(a.in_neighbors), len(a.out_neighbors)) == (len(b.out_neighbors), len(b.in_neighbors))\n",
    "#         assert a.length == b.length\n",
    "#         assert len(a.segments) == len(b.segments)\n",
    "#         assert a.total_depth == b.total_depth\n",
    "\n",
    "\n",
    "twins = sz.results.find_twins(graph2, seqs)\n",
    "sz.results.check_twins(twins, graph2, seqs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = []\n",
    "for _ in range(5):\n",
    "    test.append(tuple(tuple(p) for p in sz.assembly.iter_maximal_unitig_paths(graph2)))\n",
    "\n",
    "for i, j in product(range(5), repeat=2):\n",
    "    assert test[i] == test[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if draw_graphs:\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    sz.draw.update_xypositions(graph2, vweight=total_bases)\n",
    "    gm = sz.graph_manager.GraphManager(\n",
    "        unzippers=[\n",
    "            sz.graph_manager.LengthUnzipper(),\n",
    "            sz.graph_manager.SequenceUnzipper(),\n",
    "            sz.graph_manager.VectorDepthUnzipper(),\n",
    "            sz.graph_manager.PositionUnzipper(offset=(0.1, 0.1)),\n",
    "        ],\n",
    "        pressers=[\n",
    "            sz.graph_manager.LengthPresser(),\n",
    "            sz.graph_manager.SequencePresser(sep=\",\"),\n",
    "            sz.graph_manager.VectorDepthPresser(),\n",
    "            sz.graph_manager.PositionPresser(),\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    gm = sz.graph_manager.GraphManager(\n",
    "        unzippers=[\n",
    "            sz.graph_manager.LengthUnzipper(),\n",
    "            sz.graph_manager.SequenceUnzipper(),\n",
    "            sz.graph_manager.VectorDepthUnzipper(),\n",
    "        ],\n",
    "        pressers=[\n",
    "            sz.graph_manager.LengthPresser(),\n",
    "            sz.graph_manager.SequencePresser(sep=\",\"),\n",
    "            sz.graph_manager.VectorDepthPresser(),\n",
    "        ],\n",
    "    )\n",
    "gm.validate(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph3 = graph2.copy()  # Save for later plotting\n",
    "sz.stats.degree_stats(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 0\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth Smoothing\n",
    "\n",
    "smoothed_depths = []\n",
    "for i in range(graph2.gp['num_samples']):\n",
    "    one_depth = gt.ungroup_vector_property(graph2.vp.depth, pos=[i])[0]\n",
    "    smoothed, _change = sz.flow.smooth_depth(graph2, one_depth, graph2.vp.length, inertia=0.5, num_iter=50)\n",
    "    print(_change)\n",
    "    smoothed_depths.append(smoothed)\n",
    "\n",
    "smoothed_depths = gt.group_vector_property(smoothed_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 1\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually push smoothed depths to the graph\n",
    "# graph2.vp['depth'] = smoothed_depths  # TODO: Experiment with and without this.\n",
    "\n",
    "# FIXME: Long tips lose too much depth?\n",
    "# NOTE: It's possible that depth smoothing introduces artifacts at junctions that affects how they're split...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider dropping low depth vertices/edges\n",
    "# depth_thresh = 0.1\n",
    "# # Drop edges with low depth\n",
    "# low_depth_edge = graph2.new_edge_property('float', vals=flow.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0) < depth_thresh)\n",
    "# low_depth_edges = find_edge(graph2, low_depth_edge, True)\n",
    "# for e in low_depth_edges:\n",
    "#     graph2.remove_edge(e)\n",
    "# low_depth_vertices = idxwhere(sz.results.extract_vertex_data(graph2, seqs).total_depth < depth_thresh)\n",
    "# print(len(tips), len(low_depth_vertices), len(set(tips) & set(low_depth_vertices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim tips\n",
    "tips = sz.assembly.find_tips(graph2, also_required=graph2.vp['length'].a < graph2.gp['kmer_length'])\n",
    "print(len(tips))\n",
    "gm.batch_trim(graph2, tips)\n",
    "\n",
    "graph4 = graph2.copy()  # Save for later plotting\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all twins are intact.\n",
    "twins = find_twins(graph2, seqs)\n",
    "print(len(twins))\n",
    "check_twins(twins, graph2, seqs)\n",
    "twins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second round of tip trimming\n",
    "tips = sz.assembly.find_tips(graph2, also_required=graph2.vp['length'].a < graph2.gp['kmer_length'])\n",
    "print(len(tips))\n",
    "gm.batch_trim(graph2, tips)\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "print(len(_new_tigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all twins are intact.\n",
    "twins = find_twins(graph2, seqs)\n",
    "print(len(twins))\n",
    "check_twins(twins, graph2, seqs)\n",
    "twins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.results.extract_vertex_data(graph2, seqs).sort_values('total_depth').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 2\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result = {}\n",
    "for j in twins[1]:\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    # n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    fit, paths, named_paths, delta_aic = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=-0,\n",
    "        alpha=1.,\n",
    "        verbose=True,\n",
    "    )\n",
    "    result[j] = fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result[1].score - result[32].score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result[1].beta - result[32].beta[[1, 0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    assert np.isfinite(log_offset_ratio).all()\n",
    "    # log_offset_ratio = np.where(~np.isfinite(log_offset_ratio)\n",
    "    corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    assert np.isfinite(corrected_in_edge_flows).all()\n",
    "    assert np.isfinite(corrected_out_edge_flows).all()\n",
    "\n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        # in_edge_flows,\n",
    "        corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        # out_edge_flows,\n",
    "        corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 10):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths; score margin: {score_margin})\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_assembly_vertex_results = sz.results.extract_vertex_data(graph2, seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_assembly_vertex_results = sz.results.extract_vertex_data(graph2, seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = pre_assembly_vertex_results\n",
    "d0[(d0.segments.apply(lambda x: \"2154437+\" in x)) | (d0.segments.apply(lambda x: \"2154437-\" in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = post_assembly_vertex_results\n",
    "d0[(d0.segments.apply(lambda x: \"2154437+\" in x)) | (d0.segments.apply(lambda x: \"2154437-\" in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = post_assembly_vertex_results\n",
    "d0[(d0.segments.apply(lambda x: \"1845897+\" in x)) | (d0.segments.apply(lambda x: \"1845897-\" in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_assembly_vertex_results = sz.results.extract_vertex_data(graph2, seqs)\n",
    "d0 = pre_assembly_vertex_results\n",
    "[(j, pp, dd) for j, pp, dd in batch if ((\"1845897-\" in d0.loc[j].segments) | (\"1845897+\" in d0.loc[j].segments))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "j = 428  # 2757  # 1981  # 2760  # 3343\n",
    "\n",
    "in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "X = sz.deconvolution.design_paths(n, m)[0]\n",
    "print(X)\n",
    "\n",
    "in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "\n",
    "y_uncorrected = np.concatenate([in_edge_flows, out_edge_flows], axis=0)\n",
    "fig = plt.figure()\n",
    "sns.heatmap(y_uncorrected, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "print(log_offset_ratio)\n",
    "corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "print(corrected_in_edge_flows.sum() / corrected_out_edge_flows.sum())\n",
    "\n",
    "y = np.concatenate([corrected_in_edge_flows, corrected_out_edge_flows], axis=0)\n",
    "fig = plt.figure()\n",
    "sns.heatmap(y, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "# fig = plt.figure()\n",
    "# sns.heatmap(y - y_uncorrected, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "\n",
    "fit, paths, named_paths, delta_aic = sz.deconvolution.deconvolve_junction(\n",
    "    in_edge_vertices,\n",
    "    # in_edge_flows,\n",
    "    corrected_in_edge_flows,\n",
    "    out_edge_vertices,\n",
    "    # out_edge_flows,\n",
    "    corrected_out_edge_flows,\n",
    "    model=sz.depth_model,\n",
    "    forward_stop=0,\n",
    "    backward_stop=-0,\n",
    "    alpha=1.,\n",
    "    verbose=True,\n",
    ")\n",
    "print(named_paths, delta_aic)\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.heatmap(fit.beta, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.heatmap(fit.residual, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.heatmap(fit.stderr_beta, norm=mpl.colors.SymLogNorm(0.1, vmin=-1e2, vmax=1e2), cmap='coolwarm')\n",
    "\n",
    "print(np.linalg.cond(fit.hessian_beta))\n",
    "\n",
    "fit0 = fit\n",
    "# fit1 = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twins = find_twins(graph2, seqs)\n",
    "print(len(twins))\n",
    "check_twins(twins, graph2, seqs)\n",
    "twins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 3\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 4\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 5\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 6\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 7\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 8\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 9\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctions = sz.assembly.find_junctions(graph2)\n",
    "print(len(junctions))\n",
    "\n",
    "batch = []\n",
    "pbar = tqdm(ncols=2, disable=True)\n",
    "# for i, j in enumerate([3]):  # Trial run\n",
    "for i, j in enumerate(junctions):\n",
    "    in_edge_vertices = [edge[0] for edge in graph2.get_in_edges(j)]\n",
    "    out_edge_vertices = [edge[1] for edge in graph2.get_out_edges(j)]\n",
    "    \n",
    "    in_edge_flows = np.stack([flow[edge] for edge in graph2.get_in_edges(j)])\n",
    "    out_edge_flows = np.stack([flow[edge] for edge in graph2.get_out_edges(j)])\n",
    "    log_offset_ratio = np.log(in_edge_flows.sum()) - np.log(out_edge_flows.sum())\n",
    "    # corrected_in_edge_flows = np.exp(np.log(in_edge_flows) - log_offset_ratio / 2)\n",
    "    # corrected_out_edge_flows = np.exp(np.log(out_edge_flows) + log_offset_ratio / 2)\n",
    "    \n",
    "    n, m = len(in_edge_vertices), len(out_edge_vertices)\n",
    "    if n * m > 20:\n",
    "        print(f\"[junc={j} / {n}x{m}] Too many possible paths.\")\n",
    "        continue\n",
    "    X = sz.deconvolution.design_paths(n, m)[0]\n",
    "    pbar.set_postfix({'NxM': f\"{n}x{m}\"})\n",
    "    fit, paths, named_paths, score_margin = sz.deconvolution.deconvolve_junction(\n",
    "        in_edge_vertices,\n",
    "        in_edge_flows,\n",
    "        # corrected_in_edge_flows,\n",
    "        out_edge_vertices,\n",
    "        out_edge_flows,\n",
    "        # corrected_out_edge_flows,\n",
    "        model=sz.depth_model,\n",
    "        forward_stop=0,\n",
    "        backward_stop=0,\n",
    "        alpha=1.,\n",
    "    )\n",
    "    if not (score_margin > 20):  # TODO: Consider selecting non-best models that have a small enough score margin, after using a more negative backward_stop threshold.\n",
    "        print(f\"[junc={j} / {n}x{m}] Cannot pick best model. (Selected model had {len(paths)} paths.)\")\n",
    "    elif not X[:, paths].sum(1).min() == 1:\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-complete. (Best model had {len(paths)} paths.)\")\n",
    "    elif not len(paths) <= max(n, m):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-minimal. (Best model had {len(paths)} paths.)\")\n",
    "    elif not (np.linalg.cond(fit.hessian_beta) < 1e5):\n",
    "        print(f\"[junc={j} / {n}x{m}] Non-identifiable. (Best model had {len(paths)} paths.)\")\n",
    "    else:\n",
    "        print(f\"[junc={j} / {n}x{m}] SUCCESS! Best model had {len(paths)} paths.\")\n",
    "        batch.append((j, named_paths, {\"path_depths\": fit.beta.clip(0)}))\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "print(len(batch) / len(junctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_tigs = gm.batch_unzip(graph2, *batch)\n",
    "print(len(_new_tigs))\n",
    "\n",
    "_new_tigs = gm.batch_press(graph2, *[(path, {}) for path in sz.assembly.iter_maximal_unitig_paths(graph2)])\n",
    "len(_new_tigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz.stats.degree_stats(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = 10\n",
    "\n",
    "# Calculate Flows\n",
    "flow = []\n",
    "for sample_id in range(graph2.gp['num_samples']):\n",
    "    one_flow, _, _, = sz.flow.estimate_flow(graph2, gt.ungroup_vector_property(graph2.vp['depth'], pos=[sample_id])[0], graph2.vp['length'])\n",
    "    flow.append(one_flow)\n",
    "flow = gt.group_vector_property(flow, pos=range(graph2.gp['num_samples']))\n",
    "\n",
    "# Initial depths\n",
    "plt.hist2d(\n",
    "    graph2.vp['length'].fa,\n",
    "    graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    bins=(length_bins, depth_bins),\n",
    "    norm=mpl.colors.LogNorm(vmin=1, vmax=1e3),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f'nb/fig/component-{component}/hist_stage{assembly_stage}.pdf')\n",
    "\n",
    "if draw_graphs:\n",
    "    # Update positions\n",
    "    total_bases = graph2.new_vertex_property('float', vals=graph2.vp.length.fa * graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "    # sz.draw.update_xypositions(graph2, vweight=total_bases, max_iter=100, init_step=1)\n",
    "\n",
    "    _color = graph2.new_vertex_property('float', vals=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2))\n",
    "    _width = graph2.new_edge_property('float', vals=flow.get_2d_array(range(graph2.gp['num_samples'])).sum(0) ** (1/2) / 2)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_fill_color=_color,\n",
    "        # edge_color=flow,\n",
    "        # edge_pen_width=_width,\n",
    "        output=f'nb/fig/component-{component}/graph_stage{assembly_stage}.pdf',\n",
    "        vcmap=(mpl.cm.magma),\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def assemble_segments(segment_list, segment_to_sequence, k):\n",
    "    accum = \"\"\n",
    "    for segment in segment_list:\n",
    "        seqidx, strand = segment[:-1], segment[-1:]\n",
    "        forward_segment = segment_to_sequence[seqidx]\n",
    "        if strand == '+':\n",
    "            accum = accum[:-(k - 1)] + forward_segment\n",
    "        else:\n",
    "            accum = accum[:-(k - 1)] + sz.sequence.reverse_complement(forward_segment)\n",
    "    return accum\n",
    "\n",
    "assemble_segments([\"1003553+\",\"2022810+\",\"2022807+\"], seqs, k=graph2.gp['kmer_length'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vertex_data = pd.DataFrame(dict(\n",
    "    vertex=graph2.get_vertices(),\n",
    "    length=graph2.vp['length'],\n",
    "    total_depth=graph2.vp['depth'].get_2d_array(range(graph2.gp['num_samples'])).sum(0),\n",
    "    segments=[ss.split(',') for ss in graph2.vp['sequence']],\n",
    "    in_neighbors=[frozenset(graph2.get_in_neighbors(v)) for v in graph2.get_vertices()],\n",
    "    out_neighbors=[frozenset(graph2.get_out_neighbors(v)) for v in graph2.get_vertices()],\n",
    ")).assign(\n",
    "    segments=lambda x: x.segments.apply(tuple),\n",
    "    num_segments=lambda x: x.segments.apply(len),\n",
    "    assembly=lambda x: x.segments.apply(lambda y: assemble_segments(y, seqs, k=graph2.gp['kmer_length']))\n",
    ").set_index('vertex')\n",
    "\n",
    "assert (vertex_data.assembly.apply(len) == vertex_data.length + 110).all()\n",
    "\n",
    "vertex_data.sort_values('num_segments')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "segment_duplicity = vertex_data.segments.drop_duplicates().explode().value_counts()\n",
    "print(segment_duplicity.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deduplicated_sequences = vertex_data.sort_values('total_depth', ascending=False).reset_index().groupby('assembly').apply(lambda x: pd.Series(dict(\n",
    "    vertex=x.vertex.iloc[0],\n",
    "    length=x.length.iloc[0],\n",
    "    total_depth=x.total_depth.sum(),\n",
    "    segments=x.segments.iloc[0],\n",
    "    vertices=tuple(set(x.vertex)),\n",
    "    in_neighbors=tuple(set(chain.from_iterable(x.in_neighbors))),\n",
    "    out_neighbors=tuple(set(chain.from_iterable(x.out_neighbors))),\n",
    "    num_segments=x.num_segments.iloc[0],\n",
    "))).reset_index().set_index('vertex')\n",
    "\n",
    "deduplicated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_results0 = sz.results.extract_vertex_data(graph2, seqs)\n",
    "vertex_results = sz.results.deduplicate_vertex_data(vertex_results0)\n",
    "vertex_results.sort_values('num_segments', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_results0[lambda x: x.length == 69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_results0.segments.explode().value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = \"1750675+\"  # Focal segment/unitig\n",
    "\n",
    "vertex_results0[lambda x: x.segments.apply(lambda x: u in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'nb/fig/component-{component}/seqs_stage_final_node{u}.fn'\n",
    "with open(path, 'w') as f:\n",
    "    for vertex, d1 in vertex_results[lambda x: x.segments.apply(lambda x: u in x)].iterrows():\n",
    "        print(f\">{vertex}\\n{d1.assembly}\", file=f)\n",
    "print(path)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_stage = '_final'\n",
    "v = 2551\n",
    "\n",
    "print(v)\n",
    "print(graph2.vp.length[v])\n",
    "print(graph2.vp.depth[v])\n",
    "print(graph2.vp.sequence[v])\n",
    "print()\n",
    "\n",
    "sns.heatmap(depth_table.sel(unitig=[int(s[:-1]) for s in graph2.vp.sequence[v].split(',')]).to_pandas().T, norm=mpl.colors.SymLogNorm(1e-1))\n",
    "\n",
    "# for segment in graph2.vp.sequence[v].split(','):\n",
    "#     print(segment, sequence_multiplicity[segment], sequence_length[segment])\n",
    "\n",
    "\n",
    "# Flag nodes in sequence v\n",
    "in_seq = graph4.new_vertex_property('bool', val=False)\n",
    "gt.map_property_values(graph4.vp.sequence, in_seq, lambda x: x in graph2.vp.sequence[v].split(','))\n",
    "\n",
    "one_depth = graph4.new_vertex_property('float', graph4.vp['depth'].get_2d_array(pos=range(graph4.gp['num_samples'])).mean(0))\n",
    "one_flow, _, _, = sz.flow.estimate_flow(graph4, one_depth, graph4.vp['length'])\n",
    "_color = graph4.new_vertex_property('float', vals=np.sqrt(one_depth.a))\n",
    "\n",
    "if draw_graphs:\n",
    "    outpath = f'nb/fig/component-{component}/graph_stage{assembly_stage}_seq{v}_id.pdf'\n",
    "    print(outpath)\n",
    "    sz.draw.draw_graph(\n",
    "        graph4,\n",
    "        vertex_text=graph4.vp['sequence'],\n",
    "        vertex_halo=in_seq,\n",
    "        # vertex_text=in_seq,\n",
    "        vertex_font_size=1,\n",
    "        vertex_fill_color=_color,\n",
    "        edge_pen_width=graph4.new_edge_property('float', vals=one_flow.a ** (1/5)),\n",
    "        output=outpath,\n",
    "        vcmap=(mpl.cm.magma, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assembly_stage = '_final'\n",
    "\n",
    "# Flag assembled sequences with component u\n",
    "u = \"1744504+\"\n",
    "has_specific_component = graph2.new_vertex_property('bool')\n",
    "gt.map_property_values(graph2.vp.sequence, has_specific_component, lambda x: u in [v for v in x.split(',')])\n",
    "\n",
    "one_depth = graph2.new_vertex_property('float', graph2.vp['depth'].get_2d_array(pos=range(graph2.gp['num_samples'])).sum(0))\n",
    "one_flow, _, _, = sz.flow.estimate_flow(graph2, one_depth, graph2.vp['length'])\n",
    "_color = graph2.new_vertex_property('float', vals=np.sqrt(one_depth.a))\n",
    "\n",
    "if draw_graphs:\n",
    "    outpath = f'nb/fig/component-{component}/graph_stage{assembly_stage}_node{u}.pdf'\n",
    "    print(outpath)\n",
    "    sz.draw.draw_graph(\n",
    "        graph2,\n",
    "        vertex_text=graph2.vp['length'],\n",
    "        vertex_halo=has_specific_component,\n",
    "        # vertex_text=in_seq,\n",
    "        vertex_fill_color=_color,\n",
    "        edge_pen_width=graph2.new_edge_property('float', vals=one_flow.a ** (1/5)),\n",
    "        output=outpath,\n",
    "        vcmap=(mpl.cm.magma, 1),\n",
    "    )\n",
    "\n",
    "print(np.where(has_specific_component.a)[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = f'nb/fig/component-{component}/seqs_stage{assembly_stage}_node{u}.fn'\n",
    "print(path)\n",
    "\n",
    "seen_segment_string = set()\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    for seq in np.where(has_specific_component.a)[0]:\n",
    "        accum = \"\"\n",
    "        segment_str = graph2.vp.sequence[seq]\n",
    "        for segment in segment_str.split(','):\n",
    "            # print(segment)\n",
    "            seqidx, strand = segment[:-1], segment[-1:]\n",
    "            forward_segment = seqs[seqidx]\n",
    "            # print(len(forward_segment))\n",
    "            if strand == '+':\n",
    "                accum = accum[:-(k - 1)] + forward_segment\n",
    "            else:\n",
    "                accum = accum[:-(k - 1)] + sz.sequence.reverse_complement(forward_segment)\n",
    "\n",
    "        in_neighbors = graph2.get_in_neighbors(seq)\n",
    "        out_neighbors = graph2.get_out_neighbors(seq)\n",
    "        print(seq, segment_str, len(accum), f\"{in_neighbors} X {out_neighbors}\", sep='\\t')\n",
    "        if segment_str in seen_segment_string:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\">{seq}\\n{accum}\", file=f)\n",
    "            seen_segment_string |= {segment_str}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = pd.DataFrame(graph2.vp.depth.get_2d_array(pos=range(graph2.gp['num_samples']))[:,np.where(has_specific_component.fa)[0]], columns=np.where(has_specific_component.a)[0])\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(d, norm=mpl.colors.SymLogNorm(1e-2), xticklabels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strainzip",
   "language": "python",
   "name": "strainzip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}